# Usage

## Download ids for ZTF fields/CCDs/quadrants

- Create CSV file for single CCD/quad pair in a field:

```sh
./get_quad_ids.py --catalog ZTF_source_features_DR5 --field 301 --ccd 2 --quad 3 --minobs 20 --skip 0 --limit 10000
```

- Create multiple HDF5 files for some CCD/quad pairs in a field:

```sh
./get_quad_ids.py --catalog ZTF_source_features_DR5 --field 301 --multi-quads --ccd-range 1 8 --quad-range 2 4 --minobs 20 --limit 10000
```

- Create multiple HDF5 files for all CCD/quad pairs in a field:

```sh
./get_quad_ids.py --catalog ZTF_source_features_DR5 --field 301 --multi-quads --minobs 20 --limit 10000
```

- Create single HDF5 file for all sources in a field:

```sh
./get_quad_ids.py --catalog ZTF_source_features_DR5 --field 301 --whole-field
```

## Training deep learning models

For details on the SCoPe taxonomy and architecture,
please refer to [arxiv:2102.11304](https://arxiv.org/pdf/2102.11304.pdf).

- The training pipeline can be invoked with the `scope.py` utility. For example:

```sh
./scope.py train --tag=vnv --path_dataset=data/training/dataset.d15.csv --batch_size=64 --epochs=100 --verbose=1 --pre_trained_model=models/experiment/vnv/vnv.20221117_001502.h5
```

Refer to `./scope.py train --help` for details.

- All the necessary metadata/configuration could be defined in `config.yaml` under `training`,
but could also be overridden with optional `scope.py train` arguments, e.g.
`./scope.py train ... --batch_size=32 --threshold=0.6 ...`.

- The pipeline uses the `ScopeNet` models defined in `scope.nn` as subclassed `tf.keras.models.Model`'s.
- The `Dataset` class defined in `scope.utils` hides the complexity of our dataset handling "under the rug".
- Datasets and pre-trained models could be fetched from the GCS with the `scope.py` tool:

```sh
./scope.py fetch-datasets
./scope.py fetch-models
```

  This requires permissions to access the `gs://ztf-scope` bucket. Alternatively, you can request access to a Google Drive folder containing the latest trained models [here](https://drive.google.com/drive/folders/1_oLBxveioKtw7LyMJfism745USe9tEGZ?usp=sharing).

- Feature name sets are specified in `config.yaml` under `features`.
  These are referenced in `config.yaml` under `training.classes.<class>.features`.

- Feature stats to be used for feature scaling/standardization before training
  is defined in `config.yaml` under `feature_stats`.

- We use [Weights & Biases](https://wandb.com) to track experiments.
  Project details and access credentials can be defined in `config.yaml` under `wandb`.

An example `bash` script to train all classifier families:

```sh
for class in pnp longt i fla ew eb ea e agn bis blyr ceph dscu lpv mir puls rrlyr rscvn srv wuma yso; \
  do echo $class; \
  for state in 1 2 3 4 5 6 7 8 9 42; \
    do ./scope.py train \
      --tag=$class --path_dataset=data/training/dataset.d15.csv \
      --scale_features=min_max --batch_size=64 \
      --epochs=300 --patience=30 --random_state=$state \
      --verbose=1 --gpu=1 --conv_branch=true --save; \
  done; \
done;
```

If `--save` is specified during training, an HDF5 file of the model's layers and weights will be saved. This file can be directly used for additional training and inferencing.

A training script containing one line per class to be trained can be generated by running `./scope.py create_training_script`, for example:
```bash
./scope.py create_training_script --filename='train_dnn.sh' --min_count=100 --pre_trained_group_name='experiment' --add_keywords='--save --batch_size=32 --group=new_experiment --period_suffix=ELS_ECE_EAOV'
```
A path to the training set may be provided as input to this method or otherwise taken from `config.yaml` (`training: dataset:`). To continue training on existing models, specify the `--pre_trained_group_name` keyword containing the models in `create_training_script`. If training on a feature collection containing multiple sets of periodic features (from different algorithms), set the suffix corresponding to the desired algorithm using `--period_suffix` or the `features: info: period_suffix:` field in the config file. The string specified in  `--add_keywords` serves as a catch-all for additional keywords that the user wishes to be included in each line of the script.

If `--pre_trained_group_name` is specified and the `--train_all` keyword is set, the output script will train all classes specified in `config.yaml` regardless of whether they have a pre-trained model. If `--train_all` is not set (the default), the script will limit training to classes that have an existing trained model.

## Running inference

Running inference requires the following steps: download ids of a field, download (or generate) features for all downloaded ids, run inference for all available trained models, e.g:
```
./tools/get_quad_ids.py --field=<field_number> --whole_field
./tools/get_features.py --field=<field_number> --whole_field --impute_missing_features
```
OR
```
./tools/generate_features.py --field <field_number> --ccd <ccd_number> --quad <quad_number> --doGPU
```

The optimal way to run inference is through an inference script generated by running `./scope.py create_inference_script` with the appropriate arguments. After creating the script and adding the needed permissions (e.g. using `chmod +x`), the commands to run inference on the field `<field_number>` are (in order):
```
./get_all_preds.sh <field_number>
```

* Requires `models_dnn/` or `models_xgb/` folder in the root directory containing the pre-trained models for DNN and XGBoost, respectively.
* In a `preds_dnn` or `preds_xgb` directory, creates a single `.parquet` (and optionally `.csv`) file containing all ids of the field in the rows and inference scores for different classes across the columns.
* If running inference on specific ids instead of a field/ccd/quad (e.g. on GCN sources), run `./get_all_preds.sh specific_ids`

## Handling different file formats
When our manipulations of `pandas` dataframes is complete, we want to save them in an appropriate file format with the desired metadata. Our code works with multiple formats, each of which have advantages and drawbacks:

- <b>Comma Separated Values (CSV, .csv):</b> in this format, data are plain text and columns are separated by commas. While this format offers a high level of human readability, it also takes more space to store and a longer time to write and read than other formats.

  `pandas` offers the `read_csv()` function and `to_csv()` method to perform I/O operations with this format. Metadata must be included as plain text in the file.

- <b>Hierarchical Data Format (HDF5, .h5):</b> this format stores data in binary form, so it is not human-readable. It takes up less space on disk than CSV files, and it writes/reads faster for numerical data. HDF5 does not serialize data columns containing structures like a `numpy` array, so file size improvements over CSV can be diminished if these structures exist in the data.

  `pandas` includes `read_hdf()` and `to_hdf()` to handle this format, and they require a package like [`PyTables`](https://www.pytables.org/) to work. `pandas` does not currently support the reading and writing of metadata using the above function and method. See `scope/utils.py` for code that handles metadata in HDF5 files.

- <b>Apache Parquet (.parquet):</b> this format stores data in binary form like HDF5, so it is not human-readable. Like HDF5, Parquet also offers significant disk space savings over CSV. Unlike HDF5, Parquet supports structures like `numpy` arrays in data columns.

  While `pandas` offers `read_parquet()` and `to_parquet()` to support this format (requiring e.g. [`PyArrow`](https://arrow.apache.org/docs/python/) to work), these again do not support the reading and writing of metadata associated with the dataframe.  See `scope/utils.py` for code that reads and writes metadata in Parquet files.

## Mapping between column names and Fritz taxonomies
The column names of training set files and Fritz taxonomy classifications are not the same by default. Training sets may also contain columns that are not meant to be uploaded to Fritz. To address both of these issues, we use a 'taxonomy mapper' file to connect local data and Fritz taxonomies.

This file must currently be generated manually, entry by entry. Each entry's key corresponds to a column name in the local file. The set of all keys is used to establish the columns of interest for upload or download. For example, if the training set includes columns that are not classifications, like RA and Dec, these columns should not be included among the entries in the mapper file. The code will then ignore these columns for the purpose of classification.

The fields associated with each key are `fritz_label` (containing the associated Fritz classification name) and `taxonomy_id` identifying the classification's taxonomy system. The mapper must have the following format, also demonstrated in `golden_dataset_mapper.json` and `DNN_AL_mapper.json`:

```
{
"variable":
    {"fritz_label": "variable",
      "taxonomy_id": 1012
    },

"periodic":
    {"fritz_label": "periodic",
      "taxonomy_id": 1012
    },

    .
    . [add more entries here]
    .

"CV":
    {"fritz_label": "Cataclysmic",
      "taxonomy_id": 1011
    }
}

```

## Generating features
Code has been adapted from [ztfperiodic](https://github.com/mcoughlin/ztfperiodic) and other sources to calculate basic and Fourier stats for light curves along with other features. This allows new features to be generated with SCoPe, both locally and using GPU cluster resources. The feature generation script is contained within `tools/generate_features.py`.

Currently, the basic stats are calculated via `tools/featureGeneration/lcstats.py`, and a host of period-finding algorithms are available in `tools/featureGeneration/periodsearch.py`. Among the CPU-based period-finding algorithms, there is not yet support for `AOV_cython`. For the `AOV` algorithm to work, run `source build.sh` in the `tools/featureGeneration/pyaov/` directory, then copy the newly created `.so` file (`aov.cpython-310-darwin.so` or similar) to `lib/python3.10/site-packages/` or equivalent within your environment. The GPU-based algorithms require CUDA support (so Mac GPUs are not supported).

inputs:
1. --source_catalog* : name of Kowalski catalog containing ZTF sources (str)
2. --alerts_catalog* : name of Kowalski catalog containing ZTF alerts (str)
3. --gaia_catalog* : name of Kowalski catalog containing Gaia data (str)
4. --bright_star_query_radius_arcsec : maximum angular distance from ZTF sources to query nearby bright stars in Gaia (float)
5. --xmatch_radius_arcsec : maximum angular distance from ZTF sources to match external catalog sources (float)
6. --kowalski_instances* : dictionary containing {names of Kowalski instances : authenticated penquins.Kowalski objects} (dict)
7. --limit : maximum number of sources to process in batch queries / statistics calculations (int)
8. --period_algorithms* : dictionary containing names of period algorithms to run. Normally specified in config - if specified here, should be a (list)
9. --period_batch_size : maximum number of sources to simultaneously perform period finding (int)
10. --doCPU : flag to run config-specified CPU period algorithms (bool)
11. --doGPU : flag to run config-specified GPU period algorithms (bool)
12. --samples_per_peak : number of samples per periodogram peak (int)
13. --doScaleMinPeriod : for period finding, scale min period based on min_cadence_minutes (bool). Otherwise, set --max_freq to desired value
14. --doRemoveTerrestrial : remove terrestrial frequencies from period-finding analysis (bool)
15. --Ncore : number of CPU cores to parallelize queries (int)
16. --field : ZTF field to run (int)
17. --ccd : ZTF ccd to run (int)
18. --quad : ZTF quadrant to run (int)
19. --min_n_lc_points : minimum number of points required to generate features for a light curve (int)
20. --min_cadence_minutes : minimum cadence between light curve points. Higher-cadence data are dropped except for the first point in the sequence (float)
21. --dirname : name of generated feature directory (str)
22. --filename : prefix of each feature filename (str)
23. --doCesium : flag to compute config-specified cesium features in addition to default list (bool)
24. --doNotSave : flag to avoid saving generated features (bool)
25. --stop_early : flag to stop feature generation before entire quadrant is run. Pair with --limit to run small-scale tests (bool)
26. --doQuadrantFile : flag to use a generated file containing [jobID, field, ccd, quad] columns instead of specifying --field, --ccd and --quad (bool)
27. --quadrant_file : name of quadrant file in the generated_features/slurm directory or equivalent (str)
28. --quadrant_index : number of job in quadrant file to run (int)
29. --doSpecificIDs: flag to perform feature generation for ztf_id column in config-specified file (bool)
30. --skipCloseSources: flag to skip removal of sources too close to bright stars via Gaia (bool)
31. --top_n_periods: number of ELS, ECE periods to pass to EAOV if using ELS_ECE_EAOV algorithm (int)
32. --max_freq: maximum frequency [1 / days] to use for period finding (float). Overridden by --doScaleMinPeriod

output:
feature_df : dataframe containing generated features

\* - specified in config.yaml

### Example usage
The following is an example of running the feature generation script locally:

```
./generate_features.py --field 301 --ccd 2 --quad 4 --source_catalog ZTF_sources_20230109 --alerts_catalog ZTF_alerts --gaia_catalog Gaia_EDR3 --bright_star_query_radius_arcsec 300.0 --xmatch_radius_arcsec 2.0 --query_size_limit 10000 --period_batch_size 1000 --samples_per_peak 10 --Ncore 4 --min_n_lc_points 50 --min_cadence_minutes 30.0 --dirname generated_features --filename gen_features --doCPU --doRemoveTerrestrial --doCesium
```

Setting `--doCPU` will run the config-specified CPU period algorithms on each source. Setting `--doGPU` instead will do likewise with the specified GPU algorithms. If neither of these keywords is set, the code will assign a value of `1.0` to each period and compute Fourier statistics using that number.

Below is an example run the script using a job/quadrant file (containing [job id, field, ccd, quad] columns) instead of specifying field/ccd/quad directly:

```
/home/bhealy/scope/tools/generate_features.py --source_catalog ZTF_sources_20230109 --alerts_catalog ZTF_alerts --gaia_catalog Gaia_EDR3 --bright_star_query_radius_arcsec 300.0 --xmatch_radius_arcsec 2.0 --query_size_limit 10000 --period_batch_size 1000 --samples_per_peak 10 --Ncore 20 --min_n_lc_points 50 --min_cadence_minutes 30.0 --dirname generated_features_DR15 --filename gen_features --doGPU --doRemoveTerrestrial --doCesium --doQuadrantFile --quadrant_file slurm.dat --quadrant_index 5738
```

### Slurm scripts
For large-scale feature generation, `generate_features.py` is intended to be run on a high-performance computing cluster. Often these clusters require jobs to be submitted using a utility like `slurm` (Simple Linux Utility for Resource Management) to generate scripts. These scripts contain information about the type, amount and duration of computing resources to allocate to the user.

Scope's `generate_features_slurm.py` code creates two slurm scripts: (1) runs single instance of `generate_features.py`, and (2) runs the `generate_features_job_submission.py` which submits multiple jobs in parallel, periodically checking to see if additional jobs can be started. See below for more information about these components of feature generation.

`generate_features_slurm.py` can receive all of the arguments used by `generate_features.py`. These arguments are passed to the instances of feature generation begun by running slurm script (1). There are also additional arguments specific to cluster resource management:

inputs:
1. --job_name : name of submitted jobs (str)
2. --cluster_name : name of HPC cluster (str)
3. --partition_type : cluster partition to use (str)
4. --nodes : number of nodes to request (int)
5. --gpus : number of GPUs to request (int)
6. --memory_GB : amount of memory to request in GB (int)
7. --time : amount of time before instance times out (str)
8. --mail_user: user's email address for job updates (str)
9. --account_name : name of account having HPC allocation (str)
10. --python_env_name : name of Python environment to activate before running `generate_features.py` (str)
11. --kowalski_instance_name : name of Kowalski instance containing ZTF source catalog (str)
12. --generateQuadrantFile : flag to map fields/ccds/quads containing sources to job numbers, save file (bool)
13. --max_instances : maximum number of HPC instances to run in parallel (int)
14. --wait_time_minutes : amount of time to wait between status checks in minutes (float)
15. --doSubmitLoop : flag to run loop initiating instances until out of jobs (hard on Kowalski)
16. --runParallel : flag to run jobs in parallel using slurm [recommended]. Otherwise, run in series on a single instance
17. --user : if using slurm, your username. This will be used to periodically run `squeue` and list your running jobs (str)


## Scope Download Classification
inputs:
1. --file : CSV file containing obj_id and/or ra dec coordinates. Set to "parse" to download sources by group id.
2. --group_ids : target group id(s) on Fritz for download (if CSV file not provided)
3. --start : Index or page number (if in "parse" mode) to begin downloading (optional)
4. --merge_features : Flag to merge features from Kowalski with downloaded sources
5. --features_catalog : Name of features catalog to query
6. --features_limit : Limit on number of sources to query at once
7. --taxonomy_map : Filename of taxonomy mapper (JSON format)
8. --output_dir : Name of directory to save downloaded files
9. --output_filename : Name of file containing merged classifications and features
10. --output_format : Output format of saved files, if not specified in (9). Must be one of parquet, h5, or csv.
11. --get_ztf_filters : Flag to add ZTF filter IDs (separate catalog query) to default features
12. --impute_missing_features : Flag to impute missing features using scope.utils.impute_features
13. --update_training_set : if downloading an active learning sample, update the training set with the new classification based on votes
14. --updated_training_set_prefix : Prefix to add to updated training set file
15. --min_vote_diff : Minimum number of net votes (upvotes - downvotes) to keep an active learning classification. Caution: if zero, all classifications of reviewed sources will be added

process:
1. if CSV file provided, query by object ids or ra, dec
2. if CSV file not provided, bulk query based on group id(s)
3. get the classification/probabilities/periods of the objects in the dataset from Fritz
4. append these values as new columns on the dataset, save to new file
5. if merge_features, query Kowalski and merge sources with features, saving new CSV file
6. Fritz sources with multiple associated ZTF IDs will generate multiple rows in the merged feature file
7. To skip the source download part of the code, provide an input CSV file containing columns named 'obj_id', 'classification', 'probability', 'period_origin', 'period', 'ztf_id_origin', and 'ztf_id'.
8. Set `--update_training_set` to read the config-specified training set and merge new sources/classifications from an active learning group

output: data with new columns appended.

```sh
./scope_download_classification.py --file sample.csv --group_ids 360 361 --start 10 --merge_features True --features_catalog ZTF_source_features_DR5 --features_limit 5000 --taxonomy_map golden_dataset_mapper.json --output_dir fritzDownload --output_filename merged_classifications_features --output_format parquet -get_ztf_filters --impute_missing_features
```

## Scope Download GCN Sources
inputs:
1. --dateobs: unique dateObs of GCN event (str)
2. --group_ids: group ids to query sources [all if not specified] (list)
3. --days_range: max days past event to search for sources (float)
4. --radius_arcsec: radius [arcsec] around new sources to search for existing ZTF sources (float)
5. --save_filename: filename to save source ids/coordinates (str)

process:
1. query all sources associated with GCN event
2. get fritz names, ras and decs for each page of sources
3. save json file in a useful format to use with `generate_features.py --doSpecificIDs`

```sh
./scope_download_gcn_sources.py --dateobs 2023-05-21T05:30:43
```

## Scope Upload Classification
inputs:
1. --file : path to CSV, HDF5 or Parquet file containing ra, dec, period, and labels
2. --group_ids : target group id(s) on Fritz for upload
3. --classification : Name(s) of input file columns containing classification probabilities (one column per label). Set this to "read" to automatically upload all classes specified in the taxonomy mapper at once.
4. --taxonomy_map : Filename of taxonomy mapper (JSON format)
5. --comment : Comment to post (if specified)
6. --start : Index to start uploading (zero-based)
7. --stop : Index to stop uploading (inclusive)
8. --classification_origin: origin of classifications. If 'SCoPe' (default), Fritz will apply custom color-coding
9. --skip_phot : flag to skip photometry upload (skips for existing sources only)
10. --post_survey_id : flag to post an annotation for the Gaia, AllWISE or PS1 id associated with each source
11. --survey_id_origin : Annotation origin name for survey_id
12. --p_threshold : Probability threshold for posted classification (values must be >= than this number to post)
13. --match_ids : flag to match input and existing survey_id values during upload. It is recommended to instead match obj_ids (see next line)
14. --use_existing_obj_id : flag to use existing source names in a column named 'obj_id' (a coordinate-based ID is otherwise generated by default)
15. --post_upvote : flag to post an upvote to newly uploaded classifications. Not recommended when posting automated classifications for active learning.
16. --check_labelled_box : flag to check the 'labelled' box for each source when uploading classifications. Not recommended when posting automated classifications for active learning.
17. --write_obj_id : flag to output a copy of the input file with an 'obj_id' column containing the coordinate-based IDs for each posted object. Use this file as input for future uploads to add to this column.
18. --result_dir : name of directory where upload results file is saved. Default is 'fritzUpload' within the tools directory.
19. --result_filetag: name of tag appended to the result filename. Default is 'fritzUpload'.
20. --result_format : result file format; one of csv, h5 or parquet. Default is parquet.
21. --replace_classifications : flag to delete each source's existing classifications before posting new ones.

process:
0. include Kowalski host, port, protocol, and token or username+password in config.yaml
1. check if each input source exists by comparing input and existing obj_ids and/or survey_ids
2. save the objects to Fritz group if new
3. in batches, upload the classifications of the objects in the dataset to target group on Fritz
4. duplicate classifications will not be uploaded to Fritz. If n classifications are manually specified, probabilities will be sourced from the last n columns of the dataset.
5. post survey_id annotations
6. (post comment to each uploaded source)

```sh
./scope_upload_classification.py --file sample.csv --group_ids 500 250 750 --classification variable flaring --taxonomy_map map.json --comment confident --start 35 --stop 50 --skip_phot --p_threshold 0.9 --write_obj_id --result_format csv --use_existing_obj_id --post_survey_id --replace_classifications
```

## Scope Manage Annotation
inputs:
1. --action : one of "post", "update", or "delete"
2. --source : ZTF ID or path to .csv file with multiple objects (ID column "obj_id")
3. --target : group id(s) on Fritz
4. --origin : name of annotation
5. --key : name of annotation
6. --value : value of annotation (required for "post" and "update" - if source is a .csv file, value will auto-populate from `source[key]`)

process:
1. for each source, find existing annotations (for "update" and "delete" actions)
2. interact with API to make desired changes to annotations
3. confirm changes with printed messages

```sh
./scope_manage_annotation.py --action post --source sample.csv --group_ids 200 300 400 --origin revisedperiod --key period
```

## Scope Upload Disagreements
inputs:
1. dataset
2. group id on Fritz
3. gloria object

process:
1. read in the csv dataset to pandas dataframe
2. get high scoring objects on DNN or on XGBoost from Fritz
3. get objects that have high confidence on DNN but low confidence on XGBoost and vice versa
4. get different statistics of those disagreeing objects and combine to a dataframe
5. filter those disagreeing objects that are contained in the training set and remove them
6. upload the remaining disagreeing objects to target group on Fritz

```sh
./scope_upload_disagreements.py -file dataset.d15.csv -id 360 -token sample_token
```
